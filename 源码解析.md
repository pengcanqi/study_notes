# 消息存储

## 虚拟内存

### 虚拟内存产生的原因

虚拟内存是计算机系统内存管理的一种技术。它使得应用程序认为它拥有**连续可用**的内存（一个完整的地址空间），但是实际上，它通常被分割成多个物理碎片，还有部分存储在外部磁盘管理器上，必要时进行数据交换。

通过借助虚拟内存，在内存不足时仍然可以运行程序。例如，在只剩 5MB 内存空间的情况下仍然可以运行 10MB 的程序。由于 CPU 只能执行加载到内存中的程序，因此，虚拟内存的空间就需要和内存中的空间**进行置换（swap）**，然后运行程序。

### 页表

系统必须得有办法判定某个虚拟页是否缓存在主存的某个地方。这具体可分为两种情况。

- 已经在主存中，就需要判断出该虚拟页存在于哪个物理页中。
- 不在主存中，那么系统必须判断虚拟页存放在磁盘的哪个位置，并且在物理主存中选择一个牺牲页，并将该虚拟页从磁盘复制到 主存，替换这个牺牲页。

这些功能由软硬件联合提供，包括操作系统，CPU中的**内存管理单元（Memory Management Unit,MMU）**和一个存放在物理内存中叫**页表（page table）**的数据结构，页表将虚拟页映射到物理页。每次地址翻译硬件将一个虚拟地址转换成物理地址时都会读取页表。

![页表](../study_note_image/rocketmq/页表.png)

上图展示了一个页表的基本结构，页表就是一个**页表条目（Page Table Entry, PTE）**的数组。虚拟地址的每个页在页表中都有一个对应的PTE。在这里我们假设每个 PTE 是由一个有效位（Valid bit）和一个 n 位地址字段组成的。有效位表明了该虚拟页当前是否被缓存在 主存 中。

```
我们的cpu想访问虚拟地址所在的虚拟页(VP3)，根据页表，找出页表中第三条的值.判断有效位。
1. 如果有效位为1，DRMA缓存命中，根据物理页号，找到物理页当中的内容，返回。
2.若有效位为0，参数缺页异常，调用内核缺页异常处理程序。内核通过页面置换算法选择一个页面作为被覆盖的页面，将该页的内容刷新到磁盘空间当中。然后把VP3映射的磁盘文件缓存到该物理页上面。然后页表中第三条，有效位变成1，第二部分存储上了可以对应物理内存页的地址的内容。
3.缺页异常处理完毕后，返回中断前的指令，重新执行，此时缓存命中，执行1。
4.将找到的内容映射到告诉缓存当中，CPU从告诉缓存中获取该值，结束。
```

### MMAP

![内存映射](../study_note_image/rocketmq/内存映射.png)

网上很多说MMAP是把磁盘文件映射到了虚拟内存上，对虚拟内存的修改就直接是对文件的修改。对这种说法不敢苟同。上图摘自《深入理解计算机系统》内存映射部分。我觉得MMAP这样理解可能好一些：

```
传统IO，进行读写时，需要把数据进行内核态和用户态的拷贝。
通过MMAP进行的IO，多个进程的不同虚拟内存映射到了同一块物理内存上，在用户态进行修改的同时，由于内核态和用户态虚拟内存映射到了同一块物理内存，所以不用进行内核态和用户态的数据拷贝。
```

1. 用户进程通过`write()`方法发起调用，上下文从用户态转为内核态
2. CPU将应用缓冲区中数据拷贝到socket缓冲区
3. DMA控制器把数据从socket缓冲区拷贝到网卡，上下文从内核态切换回用户态，`write()`返回

### IO演进

**传统IO**

![传统IO](../study_note_image/rocketmq/传统IO.png)

**mmap**

![mmap](../study_note_image/rocketmq/mmap.png)

**sendfile**

![sendfile](../study_note_image/rocketmq/sendfile.png)

## 队列文件存储思考

**问题：MQ既然要接收大量的消息，这些消息如果全部存在内存，是否可行呢？**

在机器内存的限制下当然不行，那么就要考虑非内存的存储方式（**例如ActiveMQ是持久化到数据库中）**。

本地磁盘虽然慢（磁盘寻道），但是它的容量很大，可是如果我们顺序读写，会比随机读写性能强很多。

内存容量虽然小，但是它的速度很快。

所以，能不能有一种折中的方法，即用到内存又用到本地磁盘。

**填充和交换算法**，根据需要将固定大小（例如128M）的页面文件映射到内存中，并在固定的生存时间（TTL）时间内未访问该文件时取消映射。 通过这种设计，我不仅可以更安全，更有效地使用内存，而且可以在需要时删除一些用过的页面文件以节省磁盘空间。

**队列是一种前置读、后置追加的结构，那么只需要将队列的前部分和后部分放入内存，中间部分在磁盘操作，就能保证高效、大容量操作。**

读取和追加操作总是可以发生在内存中，这意味着入队和出队操作总是接近O(1)访问速度。

但如果想要查询在磁盘上的消息，速度还是会降下来，为此，我们可以再维护一份索引，记录目标消息在磁盘文件上的偏移量，以随机读接口去访问。

至此，一个很实用的队列文件存储系统就有眉目了。

## RocketMq的存储设计

## MappedFile

### transientStorePool

```java
public class TransientStorePool {
    private final int poolSize; //池子大小
    private final int fileSize; // 缓冲池每个缓存的大小
    private final Deque<ByteBuffer> availableBuffers;
    private final MessageStoreConfig storeConfig;
    /**
     * It's a heavy init method.
     */
    public void init() {
        for (int i = 0; i < poolSize; i++) {
            ByteBuffer byteBuffer = ByteBuffer.allocateDirect(fileSize);

            final long address = ((DirectBuffer) byteBuffer).address();
            Pointer pointer = new Pointer(address);
            /**
             *系统调用 mlock允许程序在物理内存上锁住它的部分或全部地址空间。这将阻止Linux 将这个内存页调度到			 				*交换空间swapspace），即使该程序已有一段时间没有访问这段空间,提高存储性能
             */
            LibC.INSTANCE.mlock(pointer, new NativeLong(fileSize));

            availableBuffers.offer(byteBuffer);
        }
    }
}
```

![transientStorePool](../study_note_image/rocketmq/transientStorePool.PNG)

### 刷盘策略

**同步刷盘**

```
GroupCommitService：同步刷盘策略
```

**异步刷盘**

```
FlushRealTimeService：采用mmap的异步刷盘策略
CommitRealTimeService：采用TransientStorePool+filechannel实现读写分离架构的异步刷盘策略
```

**刷盘线程唤醒策略**

在asyncPutMessages异步追加消息时（不代表仅仅是异步发送，producer同步发送也会调用此方法）：

```
CompletableFuture<PutMessageStatus> flushOKFuture = submitFlushRequest(result, messageExtBatch);
```

在putMessage同步追加消息时：

```
handleDiskFlush(result, putMessageResult, msg)
```

**同步刷盘的具体实现**

```java
class GroupCommitService extends FlushCommitLogService {
    private volatile List<GroupCommitRequest> requestsWrite = new ArrayList<GroupCommitRequest>();
    private volatile List<GroupCommitRequest> requestsRead = new ArrayList<GroupCommitRequest>();

    public synchronized void putRequest(final GroupCommitRequest request) {
        synchronized (this.requestsWrite) {
            this.requestsWrite.add(request);
        }
        this.wakeup();
    }

    private void swapRequests() {
        List<GroupCommitRequest> tmp = this.requestsWrite;
        this.requestsWrite = this.requestsRead;
        this.requestsRead = tmp;
    }
}
```

所谓的同步刷盘，就是在每次向commitLog对于mappedFile追加完消息后，通过GroupCommitService的putRequest，向requestWrite列表增加一个刷盘请求，然后就返回了。然后GroupCommitService内部的run方法，每次执行时，调用swapRequests方法，把当前累计到的刷盘请求，移动到requestsRead列表中，再逐个消费requestsRead列表中的刷盘请求。

概括来说：**同步刷盘，其实就是每次commitlog追加消息，都会往一个刷盘请求列表里扔一个刷盘请求，而后台有一个线程是在异步消费这个刷盘请求列表**

最后再来个QA环节

```
Q:为什么要这样设计同步刷盘

A:因为刷盘是IO操作很耗时间，如果producer真的要发送完一条消息，且等到消息落盘的话，就会有很多的producer阻塞。
```

**异步刷盘的具体实现**

和同步刷盘每次有消息来就往列表里仍消息不同，每次有消息追加，都会尝试唤醒异步刷盘线程（如果它处于阻塞状态）。

然后异步刷盘线程，会根据以下条件决定是刷盘亦或者继续阻塞：

```
1.脏页数是否达到阈值

2.刷盘间隔时间是否达到阈值
```

## Remoting网络通信层

### 核心类简介

![网络层类图](../study_note_image/rocketmq/网络层类图.png)

RocketMQ的网络层基于netty实现，类图如上。

NettyRomotingClient用于：producer、consumer、broker。

NettyRomotingServer用于：broker、nameServer。

我们可以看到NettyRomotingClient多了个nameServerAddressList，因为作为客户端，需要定时去主动与nameServer进行通信，获取最新的路由信息。

### NettyDecoder

```java
public class NettyDecoder extends LengthFieldBasedFrameDecoder {
    private static final InternalLogger log = InternalLoggerFactory.getLogger(RemotingHelper.ROCKETMQ_REMOTING);

    private static final int FRAME_MAX_LENGTH =
        Integer.parseInt(System.getProperty("com.rocketmq.remoting.frameMaxLength", "16777216"));

    public NettyDecoder() {
        super(FRAME_MAX_LENGTH, 0, 4, 0, 4);
    }

    @Override
    public Object decode(ChannelHandlerContext ctx, ByteBuf in) throws Exception {
        ByteBuf frame = null;
        try {
            frame = (ByteBuf) super.decode(ctx, in);
            if (null == frame) {
                return null;
            }

            ByteBuffer byteBuffer = frame.nioBuffer();

            return RemotingCommand.decode(byteBuffer);
        } catch (Exception e) {
            log.error("decode exception, " + RemotingHelper.parseChannelRemoteAddr(ctx.channel()), e);
            RemotingUtil.closeChannel(ctx.channel());
        } finally {
            if (null != frame) {
                frame.release();
            }
        }

        return null;
    }
}
```

NettyDecoder是NettyRomotingClient/NettyRomotingServer的入站解码器。把入站事件解析成RemotingCommand。而我们知道rocketmq的消息是不定长的，所以理所当然能知道RemotingCommand也是不定长的，所以我们的入站解码器是继承自LengthFieldBasedFrameDecoder，长度域解码器。

### NettyServerHandler

```java
public void processMessageReceived(ChannelHandlerContext ctx, RemotingCommand msg) throws Exception {
    final RemotingCommand cmd = msg;
    if (cmd != null) {

        switch (cmd.getType()) {
            case REQUEST_COMMAND:
                // 客户端发起的请求 走这里
                processRequestCommand(ctx, cmd);
                break;
            case RESPONSE_COMMAND:
                // 客户端响应给服务器的 数据 走这里
                processResponseCommand(ctx, cmd);
                break;
            default:
                break;
        }
    }
}

public enum RemotingCommandType {
    // 请求命令
    REQUEST_COMMAND,
    // 回复命令
    RESPONSE_COMMAND;
}
```

nettyServerHandler是NettyRomotingClient/NettyRomotingServer中的一个inbound事件处理器，处理入站请求。入站请求，经过NettyDecoder解码后，解析成RemotingCommand。RemotingCommand又分为请求命令和回复命令两种类型，并根据两种命令进行不同的逻辑处理。

### 请求命令与回复命令

```
请求命令：客户端向服务端请求，不需要知晓响应结果，这个时候服务端接收到的命令为请求命令。

回复命令：客户端c1向服务端s2请求，需要知晓服务端s2的响应结果，服务端s2处理完命令请求，拿到响应结果，又作为客户端c2，向C1发送响应结果，这个时候c1作为S1，接受到的请求就是响应请求。
```

典型的交互如下所示：

![请求命令和回复命令](../study_note_image/rocketmq/请求命令和回复命令.png)

请求命令处理流程：

- 根据命令号，在处理器表中找到处理器
- 处理器处理事件
- 执行处理回调，写回响应（可选项）

回复命令处理流程：

- 根据回复命令的的opaque，在responseTable找到对应的responseFuture
- 将RemotingCommand中的reponse设置到responseFuture中
- 唤醒正在等待的responseFuture响应的线程
- 执行回调，处理response数据

### scanResponseTable

```java
public void scanResponseTable() {
    final List<ResponseFuture> rfList = new LinkedList<ResponseFuture>();
    Iterator<Entry<Integer, ResponseFuture>> it = this.responseTable.entrySet().iterator();
    while (it.hasNext()) {
        Entry<Integer, ResponseFuture> next = it.next();
        ResponseFuture rep = next.getValue();

        if ((rep.getBeginTimestamp() + rep.getTimeoutMillis() + 1000) <= System.currentTimeMillis()) {
            rep.release();
            it.remove();
            rfList.add(rep);
            log.warn("remove timeout request, " + rep);
        }
    }

    for (ResponseFuture rf : rfList) {
        try {
            executeInvokeCallback(rf);
        } catch (Throwable e) {
            log.warn("scanResponseTable, operationComplete Exception", e);
        }
    }
}
```

scanResponseTable是NettyRomotingClient/NettyRomotingServer的核心定时任务之一，随着NettyRomotingClient/NettyRomotingServer启动而启动，每1s执行一次。扫描 responseTable 表，将过期的 responseFuture 移除。

NettyRomotingClient中的responseTable：拉取消息，发送同步消息，获取路由信息等操作。

NettyRomotingServer中的responseTable：broker主动向客户端服务端通知等操作。（其实broker里面也有NettyRomotingClient啊，感觉也可以用NettyRomotingClient实现）

### 同步调用与回执

![同步调用](../study_note_image/rocketmq/同步调用.png)

### 异步调用与回执

![异步请求](../study_note_image/rocketmq/异步请求.png)

### 单向与回执

单向调用与异步调用类似，发送消息就立即调用返回了。但是与异步调用不同的是，单向调用并不关心处理结果，而只关心是否发送结果。

## NameServer

NameServer主要包括两个功能：**Broker管理**，NameServer接受Broker集群的注册信息并且保存下来作为路由信息的基本数据。然后提供心跳检测机制，检查Broker是否还存活；**路由信息管理**，每个NameServer将保存关于Broker集群的整个路由信息和用于客户端查询的队列信息。

### RouteInfoManager

Broker管理/路由信息管理功能都通过RouteInfoManager来处理，RouteInfoManager维护了理由信息和broker存活信息的map。

```java
public class RouteInfoManager {
    private static final InternalLogger log = InternalLoggerFactory.getLogger(LoggerName.NAMESRV_LOGGER_NAME);
    private final static long BROKER_CHANNEL_EXPIRED_TIME = 1000 * 60 * 2;
    private final ReadWriteLock lock = new ReentrantReadWriteLock();
    private final HashMap<String/* topic */, List<QueueData>> topicQueueTable;
    private final HashMap<String/* brokerName */, BrokerData> brokerAddrTable;
    private final HashMap<String/* clusterName */, Set<String/* brokerName */>> clusterAddrTable;
    private final HashMap<String/* brokerAddr */, BrokerLiveInfo> brokerLiveTable;
    private final HashMap<String/* brokerAddr */, List<String>/* Filter Server */> filterServerTable;
}
```

### Broker管理

#### 核心类介绍

```java
class BrokerLiveInfo {
    private long lastUpdateTimestamp; //上一次心跳更新时间
    private DataVersion dataVersion; // broker版本信息
    private Channel channel;
    private String haServerAddr;
}

public class DataVersion extends RemotingSerializable {
    private long timestamp = System.currentTimeMillis();
    private AtomicLong counter = new AtomicLong(0);// 自增版本序列号

    public void assignNewOne(final DataVersion dataVersion) {
        this.timestamp = dataVersion.timestamp;
        this.counter.set(dataVersion.counter.get());
    }

    //当broker信息有变化时，会调用nextVersion方法
    public void nextVersion() {
        this.timestamp = System.currentTimeMillis();
        this.counter.incrementAndGet();
    }
}
```

#### scanNotActiveBroker

```java
    public void scanNotActiveBroker() {
        Iterator<Entry<String, BrokerLiveInfo>> it = this.brokerLiveTable.entrySet().iterator();
        while (it.hasNext()) {
            Entry<String, BrokerLiveInfo> next = it.next();
            long last = next.getValue().getLastUpdateTimestamp();
            if ((last + BROKER_CHANNEL_EXPIRED_TIME) < System.currentTimeMillis()) {
                RemotingUtil.closeChannel(next.getValue().getChannel());
                it.remove();
                log.warn("The broker channel expired, {} {}ms", next.getKey(), BROKER_CHANNEL_EXPIRED_TIME);
                // 参数1：brokerAddr
                // 参数2：服务器与broker物理节点的 ch
                this.onChannelDestroy(next.getKey(), next.getValue().getChannel());
            }
        }
    }
```

随着nameServer启动而启动的定时任务,每10s检测一次broker存活信息，如果上次心跳更新时间距离现在大于2分钟则broker下线。

而brokerLiveTable的更新是在broker启动的时候，会启动一个定时任务，执行registerBrokerAll（）方法，如果判断DataVersion没有改变，就更新心跳时间。

### 路由信息管理

![元数据](C:\Users\pengc\Desktop\study\study_note_image\rocketmq\元数据.png)

路由信息由RouteInfoManager在的topicQueueTable和brokerAddrTable来维护。总体来说就是一个topic对应多个QueueData，每个QueueData对应一个broker主从机器集合。而客户端从NameServer中获取到的路由信息如下所示：


```
public class TopicRouteData extends RemotingSerializable {
    private String orderTopicConf;
    private List<QueueData> queueDatas;
    private List<BrokerData> brokerDatas;
    private HashMap<String/* brokerAddr */, List<String>/* Filter Server */> filterServerTable;
}
```

### 读写锁在NameServer的使用

可能会存在同时需要取路由信息和修改理由信息的情况。在NameServer中，通过lock字段，即通过读写锁，解决并发读写路由信息的问题。

## Producer消息生产者

### 设计总览

![客户端设计图（生产者角度）](../study_note_image/rocketmq/客户端设计图（生产者角度）.jpg)

### MQClientInstance

MQClientInstance对象在RocketMQ中非常重量级，所有通信请求都会优先由它来处理，包括拉取请求、开线程定时刷新本地缓存数据、消费负载、心跳检测等等，因此针对一个MQ集群，如果创建多个这样的实例，很可能会导致一系列并发问题。所以RocketMQ在底层维护了一个Map来缓存该实例（Key为客户端clientId，Value为客户端实例）。

```java
// 客户端实例管理器
public class MQClientManager {
    private final static InternalLogger log = ClientLogger.getLog();
    private static MQClientManager instance = new MQClientManager();
    private AtomicInteger factoryIndexGenerator = new AtomicInteger();
    // 管理客户端实例
    private ConcurrentMap<String/* clientId */, MQClientInstance> factoryTable =
        new ConcurrentHashMap<String, MQClientInstance>();
}
```

#### clientId生成

```java
public class ClientConfig {
    public String buildMQClientId() {
        StringBuilder sb = new StringBuilder();
        sb.append(this.getClientIP());

        sb.append("@");
        sb.append(this.getInstanceName());
        if (!UtilAll.isBlank(this.unitName)) {
            sb.append("@");
            sb.append(this.unitName);
        }

        return sb.toString();
    }
}
```

如果为指定instanceName，会采用默认的instanceName “DEFAULT”，则clientId为 ip@DEFAULT

#### 生产者/消费者注册

![MqClientInstance](../study_note_image/rocketmq/MqClientInstance.png)

MqClientInstance的配置由的ClientConfig类维护，MqClientInstance含有一个ClientConfig的引用clientConfig。所以MqClientInstance一般对应了一个MQ集群。多个生产者消费者线程都可以注册到同一个MqClientInstance中，MqClientInstance中有维护关于消费者和生产者的ConcurrentHashMap，其中key是group的名称。

#### RocketMQ 多个实例消息配置

如果项目中需要同时消费多个不同集群的消息。注意一定要配置不同的instanceName，否则就算配置了不同的nameServer，但是由于生成的clientId是相同的，消费者启动时，拿到的MqClientInstance是同一个，永远都是第一个启动的MqClientInstance，导致配置的不同的nameServer其实并不会生效。

#### 核心定时任务

有如下核心定时任务随MQClientInstance启动而启动：

------

定时任务1：从nameServer更新路由信息

周期：30s一次

```java
// 定时任务1：从nameserver更新 客户端 本地的 路由数据
// 周期：30秒一次
this.scheduledExecutorService.scheduleAtFixedRate(new Runnable() {

    @Override
    public void run() {
        try {
            MQClientInstance.this.updateTopicRouteInfoFromNameServer();
        } catch (Exception e) {
            log.error("ScheduledTask updateTopicRouteInfoFromNameServer exception", e);
        }
    }
}, 10, this.clientConfig.getPollNameServerInterval(), TimeUnit.MILLISECONDS);
```

------

定时任务2：
事情1：清理下线的broker节点
事情2：向在线的broker节点发送 心跳数据
周期：30秒一次

```java
this.scheduledExecutorService.scheduleAtFixedRate(new Runnable() {

    @Override
    public void run() {
        try {
            // 事情1：清理下线的broker节点，检查客户端 路由表 ，将路由表中 不包含的 addr 清理掉，如果被清理的 brokerName 下 所有的
            // 物理节点 都没有了 的 话，需要将 broker 映射数据 删掉。
            MQClientInstance.this.cleanOfflineBroker();
            // 事情2：向在线的broker节点发送 心跳数据
            MQClientInstance.this.sendHeartbeatToAllBrokerWithLock();
        } catch (Exception e) {
            log.error("ScheduledTask sendHeartbeatToAllBroker exception", e);
        }
    }
}, 1000, this.clientConfig.getHeartbeatBrokerInterval(), TimeUnit.MILLISECONDS);
```

------

定时任务3：消费者持久化 消费进度 
周期：5秒一次

```java
this.scheduledExecutorService.scheduleAtFixedRate(new Runnable() {

    @Override
    public void run() {
        try {
            MQClientInstance.this.persistAllConsumerOffset();
        } catch (Exception e) {
            log.error("ScheduledTask persistAllConsumerOffset exception", e);
        }
    }
}, 1000 * 10, this.clientConfig.getPersistConsumerOffsetInterval(), TimeUnit.MILLISECONDS);
```

### 自动创建主题

#### 消费者向不存在主题发送消息

![自动创建主题](../study_note_image/rocketmq/自动创建主题.png)

自动创建主题流程如上图所示。

当producer尝试向一个不存在的topic发送消息时，发送程序会报错：**No route info of this topic**。但是broker有个配置项**autoCreateTopicEnable**，当开启时，producer可以向一个不存在的topic发送消息，broker在收到消息后会自动创建主题。如上图所示，当autoCreateTopicEnable关闭时，无论从哪里都获取不到test/TWB102这个主题的路由信息，就会抛出No route info of this topic的异常。

#### 为什么生产上不建议自动创建主题

为了消息发送的高可用，希望新创建的Topic在集群中的每台Broker上创建对应的队列，避免Broker的单节点故障。而自动创建主题，得是broker收到了这个不存在的主题的消息，才会自动创建。那另一个broker要是没收到，那相当于这个topic就不会在这个broker上创建。

家陆的疑问：那他妈的他为啥不收到了一个topic创建请求做一下同步？

我个人的见解：同步这个得nameserver来做了，但是nameserver的设计又是轻量级的，是ap的，由它来做这个操作不符合设计初衷。

### 消息重投

#### 同步消息重投

![同步消息重投](../study_note_image/rocketmq/同步消息重投.png)

```java
private int retryTimesWhenSendFailed = 2;
private boolean retryAnotherBrokerWhenNotStoreOK = false;
```

同步消息重投由上述两个变量控制，重投逻辑如上图所示，简单来说就是for循环里发送消息，for循环的循环次数就是最大重试次数，发送失败了就重试，发送成功了就返回发送结果。

同步发送失败重投次数，默认为2，因此生产者会最多尝试发送retryTimesWhenSendFailed +  1次。**不会选择上次失败的broker，尝试向其他broker发送，最大程度保证消息不丢**。超过重投次数，抛出异常，由客户端保证消息不丢。当出现RemotingException、MQClientException和部分MQBrokerException时会重投。

#### 异步消息重投

![异步消息重投](../study_note_image/rocketmq/异步消息重投.png)

retryTimesWhenSendAsyncFailed:异步发送失败重试次数，**异步重试不会选择其他broker，仅在同一个broker上做重试，不保证消息不丢**。

### Queue队列选择策略

#### 默认投递方式：基于`Queue队列`轮询算法投递

```java
/**
 * 默认选择队列的方法
 * 参数1：上次失败的BrokerName（可以为null）
 * 返回值：当前主题下的 一个 队列
 */
public MessageQueue selectOneMessageQueue(final String lastBrokerName) {
    if (lastBrokerName == null) {
        return selectOneMessageQueue();
    } else {
        for (int i = 0; i < this.messageQueueList.size(); i++) {
            int index = this.sendWhichQueue.getAndIncrement();
            int pos = Math.abs(index) % this.messageQueueList.size();
            if (pos < 0)
                pos = 0;
            MessageQueue mq = this.messageQueueList.get(pos);
            if (!mq.getBrokerName().equals(lastBrokerName)) {
                return mq;
            }
        }
        return selectOneMessageQueue();
    }
}
```

默认情况下，采用了最简单的轮询算法，这种算法有个很好的特性就是，保证每一个`Queue队列`的消息投递数量尽可能均匀。

当消息重投时，会选择与lastBrokerName不同的brokerName中的`Queue队列`。

#### 默认投递方式的增强：基于`Queue队列`轮询算法和`消息投递延迟最小`的策略投递

默认的投递方式比较简单，但是也暴露了一个问题，就是有些`Queue队列`可能由于自身数量积压等原因，可能在投递的过程比较长，对于这样的`Queue队列`会影响后续投递的效果。 基于这种现象，RocketMQ在每发送一个MQ消息后，都会统计一下消息投递的`时间延迟`，根据这个`时间延迟`，可以知道往哪些`Queue队列`投递的速度快。 在这种场景下，会优先使用`消息投递延迟最小`的策略，如果没有生效，再使用`Queue队列轮询`的方式。



### 消息投递流程

![消息发送](../study_note_image/rocketmq/消息发送.png)

无论是哪种消息投递方式，都基本遵循如下流程：

1.根据topic获取到路由信息

2.选择队列

3.发送消息

需要注意的是

## Consumer消息消费者

### 延时消息

![延时消息](C:\Users\pengc\Desktop\study\study_note_image\rocketmq\延时消息.png)

定时消息是指消息发送到broker后，不会立即被消费，等待特定时间投递给真正的topic。 broker有配置项messageDelayLevel，messageDelayLevel是broker的属性，不属于某个topic。默认的延时等级和延时时间对照关系如下：

| 1    | 2    | 3    | 4    | 5    | 6    | 7    | 8    | 9    | 10   | 11   | 12   | 13   | 14   | 15   | 16   | 17   | 18   |
| ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- |
| 1s   | 5s   | 10s  | 30s  | 1m   | 2m   | 3m   | 4m   | 5m   | 6m   | 7m   | 8m   | 9m   | 10m  | 20m  | 30m  | 1h   | 2h   |

定时消息会暂存在名为SCHEDULE_TOPIC_XXXX的topic中，并根据delayTimeLevel存入特定的queue，而延时等级和队列ID的对应关系为：`queueId = delayLevel - 1`，即一个queue只存相同延迟的消息，保证具有相同发送延迟的消息能够顺序消费。broker会调度地消费SCHEDULE_TOPIC_XXXX，将消息写入真实的topic。

负责调度地消费SCHEDULE_TOPIC_XXXX的服务为**ScheduleMessageService**，具体的调度功能由内部维护的定时器Timer负责，它负责从SHHEDULE_TOPIC_XXXX对应的queue中，取出消息，分发到真实topic对应的queue中。

ScheduleMessageService里还维护了两个重要的map：

- 延时等级和延时时间映射
- 延时队列和对应消费位点映射（定期持久化到磁盘）

### 消费重试

#### 消费消息重投

![消费重试](C:\Users\pengc\Desktop\study\study_note_image\rocketmq\消费重试.png)

RocketMQ会为每个消费组都设置一个Topic名称为“%RETRY%+consumerGroup”的重试队列（**这里需要注意的是，这个Topic的重试队列是针对消费组，而不是针对每个Topic设置的**），用于暂时保存因为各种异常而导致Consumer端无法消费的消息。考虑到异常恢复起来需要一些时间，会为重试队列设置多个重试级别，每个重试级别都有与之对应的重新投递延时，重试次数越多投递延时就越大。RocketMQ对于重试消息的处理是先保存至Topic名称为“SCHEDULE_TOPIC_XXXX”的延迟队列中，后台定时任务按照对应的时间进行Delay后重新保存至“%RETRY%+consumerGroup”的重试队列中，当达到最大重试次数后，把消息保存到死信队列中（**%DLQ%**）

默认的重试级别是：`3+consumeTimes`

也即第一次重试默认是10秒后重试。

```java
消息重试带来的问题：由于MQ的重试机制，难免会引起消息的重复消费问题。比如一个ConsumerGroup中有两个，Consumer1和Consumer2，以集群方式消费。假设一条消息发往ConsumerGroup，由Consumer1消费，但是由于Consumer1消费过慢导致超时，次数Broker会将消息发送给Consumer2去消费，这样就产生了重复消费问题。因此，使用MQ时应该对一些关键消息进行幂等去重的处理。
```

#### 并发消费重试和顺序消费重试

同步消费重试优先采用上面的重试方式，重投消息，重投失败才本地进行重试。

顺序消费重试采用阻塞本地消费进度，优先本地重试，本地重试失败再进行消息重投，避免因消息重试导致的消息顺序问题。

### 消费偏移量

#### OffsetStore

![OffsetStore](C:\Users\pengc\Desktop\study\study_note_image\rocketmq\OffsetStore.png)

消费偏移量在consumer端由类`OffsetStore`维护，它是一个接口，有如下两个实现类；

- LocalFileOffsetStore
- RemoteBrokerOffsetStore

其中LocalFileOffsetStore是广播消费使用，消费位点由consumer自己持久化。

RemoteBrokerOffsetStore是集群消费使用，消费位点由broker持久化。

广播消费这里不过多阐述，接下来所有的描述都基于集群消费。

#### 初始化消费偏移量

![初始化消费位点](C:\Users\pengc\Desktop\study\study_note_image\rocketmq\初始化消费位点.png)

在`consumer`拉取消息时，在最初准备`pullRequest`对象时，会加载消费信息的偏移量，方法为`RebalanceImpl#updateProcessQueueTableInRebalance`。其实现就是从broker端获取到消费位点信息。

#### 持久化消费偏移量

![持久化消费位点](C:\Users\pengc\Desktop\study\study_note_image\rocketmq\持久化消费位点.png)

消息消费服务ConsumeMessageService在每次消费消息后，会调用updateOffset方法，更新RemoteBrokerOffsetStore中的本地消费位点缓存offsetTable

MQClientInstance在启动时，会启动一个定时任务，5秒一次，将RemoteBrokerOffsetStore中的本地消费位点缓存offsetTable信息，发送到broker端，broker在接受到`UPDATE_CONSUMER_OFFSET`请求后，将客户端传过来的位点信息保存到ConsumerOffsetManager中的offsetTable中。需要注意的是，这个时候位点信息依旧只是保存在内存中。而具体持久化到文件，是需要broker中的定时任务，定期持久化ConsumerOffsetManager中的位点信息到文件中。

### 本地消费快照

![消费者本地快照](C:\Users\pengc\Desktop\study\study_note_image\rocketmq\消费者本地快照.png)

消息拉取下来后都会存入本地消费快照当中。

msgCount与msgSize用来做流量控制，当发现本地快照中还有很多没有被消费的消息时，就

### 消息消费与消息拉取

![消息拉取与消息消费](C:\Users\pengc\Desktop\study\study_note_image\rocketmq\消息拉取与消息消费.png)

在`MQClientInstance#start`方法中，会启动消息拉取的服务：`PullMessageService`，`PullMessageService`是`ServiceThread`的子类，启动该服务时会创建一个新的线程，我们直接来看`PullMessageService#run()`方法

```java
@Override
public void run() {
    log.info(this.getServiceName() + " service started");

    while (!this.isStopped()) {
        try {
            PullRequest pullRequest = this.pullRequestQueue.take();
            this.pullMessage(pullRequest);
        } catch (InterruptedException ignored) {
        } catch (Exception e) {
            log.error("Pull Message Service Run Method exception", e);
        }
    }

    log.info(this.getServiceName() + " service end");
}
```

在`PullMessageService#run()`方法中，该方法会从`pullRequestQueue`中获取一个`pullRequest`的操作，然后调用`this.pullMessage(pullRequest)`进行拉取操作。

在拉取操作中，会调用到`DefaultMQPushConsumerImpl`的pullMessage方法，进行以下操作：

- 计算下一次拉取位点信息，重新添加拉取请求
- 把拉取到的消息存入本地快照
- 添加消息消费请求

消息消费服务，会从消息请求队列中取出请求，进行消费操作。最终会调用MessageListener中用户自定义的回调方法。

### 并发消费

![顺序消费](../study_note_image/rocketmq/顺序消费.png)

顺序消费由ConsumeMessageOrderlyService实现，随着service启动，有一个定时任务也随之启动，定时任务调用lockMQPeriodically方法，通过reblanceImpl，向broker发送一个LOCK_BATCH_MQ请求，并更新本地快照ProcessQueue的分布式锁标记**锁定成功后，同一个consumerGroup，同时只有一个client可以对messageQueue进行消息拉取**。

而ConsumeMessageOrderlyService内部实际上就行由一个线程池去消费consumeReuqest，在consumeReuqest内部就是对msg进行消费的逻辑。每次对对应的msg消费时，需要通过MessageQueueLock锁定消息队列，**保证本地同时只有一个线程在消费对应MessageQueue的消息**。同时还要判断ProcessQueue本地快照中的分布式锁标志是否还有效，且还要获取lockConsume这把可重入锁，避免与负载均衡冲突，避免因负载均衡导致消息重新消费。

同一个consumerGroup，同时只有一个client可以对messageQueue进行消息拉取。同时只有一个线程在消费对应MessageQueue的消息。这样就保证了rocketmq的分区顺序性。

### 负载均衡

![负载均衡](../study_note_image/rocketmq/负载均衡.PNG)

**并发消费负载均衡带来的问题：**

ConsumeRequest 是一个继承了 Runnable 的类，它是消息消费核心逻辑的实现类，submitConsumeRequest 方法将 ConsumeRequest 放入 消费线程池中执行消息消费，**从它的 run 方法中可看出，如果在执行消息消费逻辑中有节点加入，重平衡后该队列被分配给其它节点进行消费了，此时的队列被丢弃，则不提交消息消费进度，因为之前已经消费了，此时就会造成消息重复消费的情况。**

```
参考：https://cloud.tencent.com/developer/article/1521811?from=article.detail.1483408
```

